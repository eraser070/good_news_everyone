{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datacleaning\n",
    "Check data, make it usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to append different categories\n",
    "l = []\n",
    "# Loop over all given csv in folder\n",
    "for file in glob('raw_data/*.csv'):\n",
    "    df = pd.read_csv(file)\n",
    "    # Get category, add column to dataframe\n",
    "    category = file.split('/')[-1].split(\"\\\\\")[-1].split('.')[0]\n",
    "    df['category'] = category\n",
    "    l.append(df)\n",
    "\n",
    "# Add all data frames together\n",
    "df = pd.concat(l, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sports           156899\n",
       "politics          87157\n",
       "world             60297\n",
       "entertainment     50282\n",
       "travel            49470\n",
       "financial         47851\n",
       "technology        41476\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out Columns\n",
    "Remove useless ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "organizations uuid thread author url ord_in_thread title locations entities highlightText language persons text external_links published crawled highlightTitle category\n"
     ]
    }
   ],
   "source": [
    "print(*df.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### organizations\n",
    "Could be useful for eda, __check later__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.organizations.unique()\n",
    "df.drop('organizations', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### thread\n",
    "Find useful information in thread!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "97132 entries could not be loaded!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# turn str to dict\n",
    "count=0\n",
    "l = []\n",
    "for s in df.thread.values:\n",
    "    s = s.replace(\"\\'\", '\\\"') # replace single quotes to double quotes because json does not support single quotes\n",
    "    try: # Try to load as json, some values are wrong\n",
    "        l.append(json.loads(s))\n",
    "    except:\n",
    "        l.append({'false': False})\n",
    "        count += 1\n",
    "print('{} entries could not be loaded!'.format(count))\n",
    "        \n",
    "df.thread = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove__ entries that could not be converted!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['false' not in i for i in  [i.keys() for i in df.thread.values]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['social', 'site_full', 'main_image', 'site_section', 'section_title', 'url', 'country', 'title', 'performance_score', 'site', 'participants_count', 'title_full', 'spam_score', 'site_type', 'published', 'replies_count', 'uuid'])"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "df.thread.values[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "sum([i['social']['pinterest']['shares'] for i in df.thread.values if 'social' in i.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social seems to be __zero__ for __every value__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'newsdump.com'"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "df.thread.values[1]['site']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['site'] = [i['site'] for i in df.thread.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "newsdump.com          71998\n",
       "wn.com                20460\n",
       "yahoo.com             15868\n",
       "cbs8.com              10907\n",
       "reuters.com            8372\n",
       "                      ...  \n",
       "zap2it.com                1\n",
       "pcmag.com                 1\n",
       "azpbs.org                 1\n",
       "nordeclair.be             1\n",
       "thefiscaltimes.com        1\n",
       "Name: site, Length: 1868, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "df.site.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Website could be __useful__ for __eda__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'CZ'"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "df.thread.values[1]['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'] = [i['country'] for i in df.thread.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "US    239304\n",
       "CZ     72003\n",
       "GB     34176\n",
       "AU     11576\n",
       "IL      9751\n",
       "        7817\n",
       "CA      4841\n",
       "IE      4495\n",
       "IN      2702\n",
       "SG      2129\n",
       "EU      2016\n",
       "CH       901\n",
       "JE       887\n",
       "EG       706\n",
       "MY       676\n",
       "IT       471\n",
       "ZA       417\n",
       "TH       327\n",
       "AE       267\n",
       "ID       211\n",
       "NZ       199\n",
       "FR       130\n",
       "HK        99\n",
       "JP        43\n",
       "CR        39\n",
       "PA        37\n",
       "DE        24\n",
       "GR        16\n",
       "TR        15\n",
       "VG        10\n",
       "VN         9\n",
       "RS         4\n",
       "BE         1\n",
       "RU         1\n",
       "Name: country, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "df.country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Country__ is useful for eda, __include__ as new __column__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'news'}"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "set([i['site_type'] for i in df.thread.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Site type only has one value, so no need to use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove__ feature __thread__ afterwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('thread', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### title\n",
    "Looks like it can be directly used like this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The Healthiest Pastas: From Quinoa to Buckwheat Noodles'"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "df.title.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'\\n' ' ' \"'\" ',' '-' '.' '0' '1' '2' '4' ':' 'A' 'B' 'C' 'D' 'F' 'H' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'V' 'W' 'Y' '[' ']' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'k' 'l' 'm' 'n' 'o' 'p' 'r' 's' 't' 'u' 'v' 'w' 'y' '£' "
     ]
    }
   ],
   "source": [
    "for i in sorted(set(str(df.title.values))):\n",
    "    print(repr(i), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### locations\n",
    "Could be __useful__ for later __eda__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.locations.values[1]\n",
    "df.drop('locations', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove tags at end of article\n",
    "df.text = [i.split('\\nTAGS')[0] for i in df.text.values]\n",
    "# Remove Copyright at end of article\n",
    "df.text = [i.split('\\nCopyright')[0] for i in df.text.values]\n",
    "# Remove Copyright as sign\n",
    "df.text = [i.split('\\n©')[0] for i in df.text.values]\n",
    "# Remove texts consisting only of *** *** ***\n",
    "df = df[[False if '*** ***' in text else True for text in df.text]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove texts with strange multiple '...' because only 10000 and we are too  lazy to do more analysis\n",
    "df['num_triple_dots'] = [len(i.split('...')) for i in df.text.values]\n",
    "df = df.query('num_triple_dots < 4')\n",
    "df.drop('num_triple_dots', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include feature text length\n",
    "df['text_length'] = [len(i) for i in df.text.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "187982"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "# Remove texts that are too long or too small\n",
    "df = df.query('text_length > 800 & text_length < 5000')\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'\\n' ' ' '\"' '$' '%' '&' \"'\" '(' ')' ',' '-' '.' '/' '0' '1' '2' '3' '4' '5' '6' '7' '8' '9' ':' ';' '?' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'R' 'S' 'T' 'U' 'V' 'W' 'Y' 'Z' '[' '\\\\' ']' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '–' '—' '’' '“' '”' "
     ]
    }
   ],
   "source": [
    "for i in sorted(set(str(df.text.values))):\n",
    "    print(repr(i), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### published\n",
    "__Useful__ for __eda__ later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2015-10-02T03:00:00.000+03:00'"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "df.published.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unnecessary columns\n",
    "__Remove__ uuid, author, url, ord_in_thread, entities, highlightText, highlightTitle, language, persons, external_links, crawled.\\\n",
    "These columns have no use for our purpose!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['uuid', 'author', 'url', 'ord_in_thread', 'entities', 'highlightText', 'highlightTitle', 'language', 'persons', 'external_links', 'crawled']\n",
    "df.drop(to_remove, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sports           50592\n",
       "financial        30173\n",
       "world            27425\n",
       "travel           26266\n",
       "technology       20972\n",
       "entertainment    20028\n",
       "politics         12526\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#### News data set ####\n# of observations: 187982\n# of features:          7\n"
     ]
    }
   ],
   "source": [
    "print('#### News data set ####')\n",
    "print('# of observations: {}'.format(df.shape[0]))\n",
    "print('# of features:          {}'.format(df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "title          4\n",
       "text           0\n",
       "published      0\n",
       "category       0\n",
       "site           0\n",
       "country        0\n",
       "text_length    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove__ nans, only 5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "source": [
    "## Try to balance out website sources\n",
    "Try to take the most possible balanced shuffle of the dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sports           50592\n",
       "financial        30173\n",
       "world            27423\n",
       "travel           26265\n",
       "technology       20971\n",
       "entertainment    20028\n",
       "politics         12526\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Create dataset for category politics\n",
      "Create dataset for category technology\n",
      "Create dataset for category sports\n",
      "Create dataset for category entertainment\n",
      "Create dataset for category financial\n",
      "Create dataset for category world\n",
      "Create dataset for category travel\n"
     ]
    }
   ],
   "source": [
    "vals_per_category = 20000\n",
    "df_balanced = []\n",
    "for category in df.category.unique():\n",
    "    print('Create dataset for category {}'.format(category))\n",
    "    df_cat = df[df.category == category]\n",
    "    # Get five publishers with most articles\n",
    "    sites = df_cat.site.unique()\n",
    "    # Split dataset\n",
    "    df_site = {}\n",
    "    list_df = []\n",
    "    # Put different sites in separated data frames\n",
    "    for site in sites:\n",
    "        df_site[site] = df_cat[df_cat.site == site]\n",
    "    # Sort sites by entries\n",
    "    _, sites = zip(*sorted(zip([df_site[site].shape[0] for site in sites], sites)))\n",
    "    # Create dataframe\n",
    "    # Fill the rest with top5\n",
    "    num_per_site = (vals_per_category)/len(sites)\n",
    "    count = len(sites)\n",
    "    for site in sites:\n",
    "        count -= 1\n",
    "        # Check if datasize is big enough\n",
    "        if df_site[site].shape[0] >= num_per_site:\n",
    "            try:\n",
    "                tmp, _ = train_test_split(df_site[site], shuffle=True, test_size=(1-num_per_site/(df_site[site].shape[0]+0.0001)), random_state=42)\n",
    "            except:\n",
    "                tmp = df_site[site][0:1]\n",
    "            list_df.append(tmp)\n",
    "        # If not add all and increase rest\n",
    "        else:\n",
    "            list_df.append(df_site[site])\n",
    "            if count > 0:\n",
    "                num_per_site += (num_per_site - df_site[site].shape[0])/count\n",
    "    df_balanced.append(pd.concat(list_df, axis=0, ignore_index=True))\n",
    "df_balanced = pd.concat(df_balanced, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "entertainment    2986\n",
       "technology       2939\n",
       "financial        2939\n",
       "world            2904\n",
       "politics         2881\n",
       "travel           2877\n",
       "sports           2819\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 87
    }
   ],
   "source": [
    "df_balanced.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test set\n",
    "With xxx values per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = 500\n",
    "# Split dataset, each category with same sample size\n",
    "TEST = []\n",
    "TRAIN = []\n",
    "for i in df.category.unique():\n",
    "    tmp = df[df['category'] == i]\n",
    "    train, test = train_test_split(tmp, shuffle=True, test_size=(xxx/(tmp.shape[0]+0.001)), random_state=42)\n",
    "    TEST.append(test)\n",
    "    TRAIN.append(train)\n",
    "df_test = pd.concat(TEST, axis=0, ignore_index=True)\n",
    "df = pd.concat(TRAIN, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "entertainment    200\n",
       "world            200\n",
       "politics         200\n",
       "technology       200\n",
       "financial        200\n",
       "sports           200\n",
       "travel           200\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "df_test.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train dataset\n",
    "Use about 20000 per category. (Already done in balancing out websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xxx = df.category.value_counts(ascending=True)[0] # number of samples per category, take all if None\n",
    "xxx = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if xxx != None:\n",
    "    # Split dataset, each category with same sample size\n",
    "    l = []\n",
    "    for i in df.category.unique():\n",
    "        tmp = df[df['category'] == i]\n",
    "        test_size = (1-xxx/(tmp.shape[0]+0.001))\n",
    "        if test_size > 0:\n",
    "            tmp, _ = train_test_split(tmp, shuffle=True, test_size=test_size, random_state=42)\n",
    "        l.append(tmp)\n",
    "    df = pd.concat(l, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "entertainment    2786\n",
       "technology       2739\n",
       "financial        2739\n",
       "world            2704\n",
       "politics         2681\n",
       "travel           2677\n",
       "sports           2619\n",
       "Name: category, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "finished in 12.70m\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "# Add functions path\n",
    "import sys\n",
    "sys.path.append('../../Functions')\n",
    "from text_lemmatization import Lemmatizer\n",
    "lemmatizer = Lemmatizer()\n",
    "start = time()\n",
    "df['text_lem'] = lemmatizer.lem_list(df.text)\n",
    "df_test['text_lem'] = lemmatizer.lem_list(df_test.text)\n",
    "print('finished in {:.2f}m'.format((time()-start)/60))\n",
    "# Took about 75 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes\n",
    "df.to_csv('dataset_categories_train.csv', index=False)\n",
    "df_test.to_csv('dataset_categories_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}